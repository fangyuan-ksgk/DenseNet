{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "c63e4f7b-6c15-45f5-9802-186774eff9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import torch\n",
    "import shutil\n",
    "import setproctitle\n",
    "import make_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6ab09c7-1b07-433e-97b8-a52fe26a584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Like you concatenate the input and output of this network at the end\n",
    "\"\"\"\n",
    "# Bottleneck network uses 2 set of BatchNorm2d+ReLU+Conv2d to expand the channels\n",
    "# to interChannels, then shrink it back to growthRate number of channels output\n",
    "# then concatenate these processed channels with the original intput channels\n",
    "# as the final output\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        # Input shape (N,nChannels,H,W), normalize each Channel by learnable\n",
    "        # mean & std value and output (N,nChannels,H,W)\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1, bias=False)\n",
    "        # output shape (N,interChannels,H,W)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        # input (N,interChannels,H,W) output (N,growthRate,H,W)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input shape (N,nChannels,H,W)\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        # concatenate in Channel dimension, final ouput shape (N,growthRate+nChannels,H,W)\n",
    "        out = torch.cat((x,out), 1)\n",
    "        return out\n",
    "    \n",
    "# SingleLayer uses 1 batchnormal + ReLU + 2d convolutional layer\n",
    "# output the transferred output concatenated with the input Channels\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        # input (N,nChannels,H,W)\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        # intput (N,nChannels,H,W)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3, padding=1, bias=False)\n",
    "        # output (N,growthRate,H,W)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x,out),1)\n",
    "        # output shape (N,nChannels+growthRate,H,W)\n",
    "        return out\n",
    "    \n",
    "# Transition shrinks the H & W by half while changing the number of Channels\n",
    "# to a pre-specified value\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1, bias=False)\n",
    "        # output shape (N,nOutChannels,H,W)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        # out shape here: (N,nOutChannels,H,W)\n",
    "        # note that default stride value for avg_pool2d is equal to kernel size\n",
    "        out = F.avg_pool2d(out,2)\n",
    "        # output shape (N,nOutChannels,H//2,W//2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "036fe5a7-77de-47b4-a8dc-f9aa682f677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            # if use bottleneck, then half the channels expansion are needed\n",
    "            nDenseBlocks //= 2\n",
    "        \n",
    "        self.nDenseBlocks = nDenseBlocks\n",
    "        nChannels = 2*growthRate\n",
    "        # input (N,nChannels,H,W) output (N,nChannels,H,W)\n",
    "        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1, bias=False)\n",
    "        # intput (N,nChannels,H,W) output (N,nChannels+nDenseBlocks*growthRate,H,W)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        # update on the new number of Channels of current output\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        # use Transition network to reduce the channels & shrink the image\n",
    "        # reduction of number of Channels according to reduction rate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.inchan_trans1 = nChannels\n",
    "        self.outchan_trans1 = nOutChannels\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "        \n",
    "        # update on number of Channels\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "        \n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        # for classification task specifially\n",
    "        # input shape (-1, nChannels) output shape (-1, nClasses)\n",
    "        self.inchan_fc = nChannels\n",
    "        self.outchan_fc = nClasses\n",
    "        self.fc = nn.Linear(nChannels,nClasses)\n",
    "                        \n",
    "        \n",
    "    \n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                # input (N,nChannels,H,W) output (N,nChannels+growthRate,H,W)\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                # input (N,nChannels,H,W) output (N,nChannels+growthRate,H,W)\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            # each time nChannels expand by growthRate (additional increment)\n",
    "            nChannels += growthRate\n",
    "        # like an easy way of getting around it\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        # x shape (N,nChannels,H,W)\n",
    "        # out shape (N,nChannels,H,W)\n",
    "        out = self.conv1(x)\n",
    "        # out shape (N,(nChannels+nDenseBlock*growthRate)*reduction,H,W) \n",
    "        out = self.trans1(self.dense1(out))\n",
    "        # keeps on increasing channels number \n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        # essentially shrink the H & W by 8 times\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        # seems to be confident that either H & W get shrinked to 1, in that case drop it\n",
    "        out = torch.squeeze(out)\n",
    "        \"Assumption is made that by now the H&W dimenison is SQUEEZED OUT!\"\n",
    "        \"Otherwise the self.fc CANNOT be applied to o|ut here!\"\n",
    "        \"So now out has shape (N, # of Channels)\"\n",
    "        out = F.log_softmax(self.fc(out),dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5439ce28-a97d-4967-b522-3f922661ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batchSz', type=int, default=64)\n",
    "parser.add_argument('--nEpochs', type=int, default=300)\n",
    "parser.add_argument('--no-cuda', action='store_true')\n",
    "parser.add_argument('--save')\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "parser.add_argument('--opt', type=str, default='sgd',\n",
    "                    choices=('sgd', 'adam', 'rmsprop'))\n",
    "# modifies to suit for Jupyter notebook implementation\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "# when args.save is undefined with any value (per initialization), use the string\n",
    "args.save = args.save or 'work/densenet.base'\n",
    "setproctitle.setproctitle(args.save)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "if os.path.exists(args.save):\n",
    "    shutil.rmtree(args.save)\n",
    "# if the directory already exist, we don't raise an error\n",
    "os.makedirs(args.save, exist_ok=True)\n",
    "\n",
    "normMean = [0.49139968, 0.48215827, 0.44653124]\n",
    "normStd = [0.24703233, 0.24348505, 0.26158768]\n",
    "normTransform = transforms.Normalize(normMean, normStd)\n",
    "\n",
    "\"\"\"\n",
    "During Training:\n",
    "1. We randomly fit subimage (flipped) to the network, the network learns to \n",
    "make accurate classification on these sub-information set.\n",
    "2. The network eventually learn 'many pathways' upon which it can make judgement\n",
    "on what class the object belongs to, based on different sub-parts of the object\n",
    "3. An interesting question is that WHY such sub-image fitting & training still work\n",
    "when we fit into it an Entire Image?\n",
    "\"\"\"\n",
    "trainTransform = transforms.Compose([\n",
    "    # randomly cropping out a 32*32 sub-image with padding\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    # by default flip horizontally with probability 0.5\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normTransform\n",
    "])\n",
    "\"\"\"\n",
    "During Testing:\n",
    "1. We simply normalize and fit into the trained network\n",
    "\"\"\"\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normTransform\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "514717bf-ec56-4c2f-bca3-7a2fad634903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, epoch, net, trainLoader, optimizer, trainF):\n",
    "    \"\"\"\n",
    "    model.train() tells your model that you are training the model. \n",
    "    So effectively layers like dropout, batchnorm etc. which behave \n",
    "    different on the train and test procedures know what is going on \n",
    "    and hence can behave accordingly.\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    nProcessed = 0\n",
    "    nTrain = len(trainLoader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \"\"\"\n",
    "        A PyTorch Variable is a wrapper around a PyTorch Tensor, and represents\n",
    "        a node in a computational graph. If x is a Variable then x.data is a\n",
    "        Tensor giving its value, and x.grad is another Variable holding the \n",
    "        gradient of x with respect to some scalar value.\n",
    "        \"\"\"\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # make_graph.save('/tmp/t.dot', loss.creator); assert(False)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        nProcessed += len(data)\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        incorrect = pred.ne(target.data).cpu().sum()\n",
    "        err = 100.*incorrect/len(data)\n",
    "        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n",
    "        print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tError: {:.6f}'.format(\n",
    "            partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),\n",
    "            loss.data, err))\n",
    "        \n",
    "        trainF.write('{},{},{}\\n'.format(partialEpoch, loss.data, err))\n",
    "        trainF.flush()\n",
    "        \n",
    "def test(agrs, epoch, net, testLoader, optimizer, testF):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    incorrect = 0\n",
    "    for data, target in testLoader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        with torch.no_grad(): \n",
    "            output = net(data)\n",
    "            test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        incorrect += pred.ne(target.data).cpu().sum()\n",
    "        \n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(testLoader) # loss function already averages over batch size\n",
    "    nTotal = len(testLoader.dataset)\n",
    "    err = 100.*incorrect/nTotal\n",
    "    print('\\nTest set: Average loss: {:.4f}, Error: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, incorrect, nTotal, err))\n",
    "\n",
    "    testF.write('{},{},{}\\n'.format(epoch, test_loss, err))\n",
    "    testF.flush()\n",
    "    \n",
    "# in-place modification of learning rate\n",
    "def adjust_opt(optAlg, optimizer, epoch):\n",
    "    if optAlg == 'sgd':\n",
    "        if epoch < 150: lr = 1e-1\n",
    "        elif epoch == 150: lr = 1e-2\n",
    "        elif epoch == 225: lr = 1e-3\n",
    "        else: return\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "260b079e-5e95-4e2c-ab21-6828f026004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers':1, 'pin_memory': True} if args.cuda else {}\n",
    "trainLoader = DataLoader(\n",
    "    dset.CIFAR10(root='cifar', train=True, download=True, transform=trainTransform),\n",
    "    batch_size = args.batchSz, shuffle=True, **kwargs)\n",
    "testLoader = DataLoader(\n",
    "    dset.CIFAR10(root='cifar', train=False, download=True, transform=testTransform),\n",
    "    batch_size=args.batchSz, shuffle=False, **kwargs)\n",
    "\n",
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5,\n",
    "                        bottleneck=True, nClasses=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e1d8ddc6-4d10-47cf-a1e3-0e96448f404a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total Number of params: 769162\n",
      "Train Epoch: 0.00 [64/50000 (0%)]\tLoss: 2.364501\tError: 90.625000\n",
      "Train Epoch: 0.00 [128/50000 (0%)]\tLoss: 2.285554\tError: 84.375000\n",
      "Train Epoch: 0.00 [192/50000 (0%)]\tLoss: 2.357407\tError: 82.812500\n",
      "Train Epoch: 0.00 [256/50000 (0%)]\tLoss: 2.225560\tError: 79.687500\n",
      "Train Epoch: 0.01 [320/50000 (1%)]\tLoss: 2.320347\tError: 87.500000\n",
      "Train Epoch: 0.01 [384/50000 (1%)]\tLoss: 2.305410\tError: 84.375000\n",
      "Train Epoch: 0.01 [448/50000 (1%)]\tLoss: 2.264392\tError: 87.500000\n",
      "Train Epoch: 0.01 [512/50000 (1%)]\tLoss: 2.346058\tError: 85.937500\n",
      "Train Epoch: 0.01 [576/50000 (1%)]\tLoss: 2.307113\tError: 85.937500\n",
      "Train Epoch: 0.01 [640/50000 (1%)]\tLoss: 2.244503\tError: 82.812500\n",
      "Train Epoch: 0.01 [704/50000 (1%)]\tLoss: 2.309759\tError: 89.062500\n",
      "Train Epoch: 0.01 [768/50000 (1%)]\tLoss: 2.076877\tError: 79.687500\n",
      "Train Epoch: 0.02 [832/50000 (2%)]\tLoss: 2.323692\tError: 81.250000\n",
      "Train Epoch: 0.02 [896/50000 (2%)]\tLoss: 1.953270\tError: 73.437500\n",
      "Train Epoch: 0.02 [960/50000 (2%)]\tLoss: 2.353092\tError: 89.062500\n",
      "Train Epoch: 0.02 [1024/50000 (2%)]\tLoss: 2.189855\tError: 78.125000\n",
      "Train Epoch: 0.02 [1088/50000 (2%)]\tLoss: 2.184191\tError: 75.000000\n",
      "Train Epoch: 0.02 [1152/50000 (2%)]\tLoss: 2.023585\tError: 78.125000\n",
      "Train Epoch: 0.02 [1216/50000 (2%)]\tLoss: 2.095353\tError: 82.812500\n",
      "Train Epoch: 0.02 [1280/50000 (2%)]\tLoss: 2.264147\tError: 84.375000\n",
      "Train Epoch: 0.03 [1344/50000 (3%)]\tLoss: 2.147315\tError: 81.250000\n",
      "Train Epoch: 0.03 [1408/50000 (3%)]\tLoss: 2.054186\tError: 76.562500\n",
      "Train Epoch: 0.03 [1472/50000 (3%)]\tLoss: 2.250275\tError: 92.187500\n",
      "Train Epoch: 0.03 [1536/50000 (3%)]\tLoss: 2.075253\tError: 81.250000\n",
      "Train Epoch: 0.03 [1600/50000 (3%)]\tLoss: 2.017724\tError: 73.437500\n",
      "Train Epoch: 0.03 [1664/50000 (3%)]\tLoss: 2.143209\tError: 76.562500\n",
      "Train Epoch: 0.03 [1728/50000 (3%)]\tLoss: 2.071646\tError: 73.437500\n",
      "Train Epoch: 0.03 [1792/50000 (3%)]\tLoss: 2.177254\tError: 84.375000\n",
      "Train Epoch: 0.04 [1856/50000 (4%)]\tLoss: 1.919816\tError: 68.750000\n",
      "Train Epoch: 0.04 [1920/50000 (4%)]\tLoss: 2.072406\tError: 75.000000\n",
      "Train Epoch: 0.04 [1984/50000 (4%)]\tLoss: 1.971327\tError: 75.000000\n",
      "Train Epoch: 0.04 [2048/50000 (4%)]\tLoss: 2.169900\tError: 75.000000\n",
      "Train Epoch: 0.04 [2112/50000 (4%)]\tLoss: 2.304409\tError: 78.125000\n",
      "Train Epoch: 0.04 [2176/50000 (4%)]\tLoss: 1.989014\tError: 76.562500\n",
      "Train Epoch: 0.04 [2240/50000 (4%)]\tLoss: 1.880435\tError: 73.437500\n",
      "Train Epoch: 0.04 [2304/50000 (4%)]\tLoss: 2.226038\tError: 82.812500\n",
      "Train Epoch: 0.05 [2368/50000 (5%)]\tLoss: 2.102829\tError: 75.000000\n",
      "Train Epoch: 0.05 [2432/50000 (5%)]\tLoss: 2.051961\tError: 84.375000\n",
      "Train Epoch: 0.05 [2496/50000 (5%)]\tLoss: 2.247618\tError: 90.625000\n",
      "Train Epoch: 0.05 [2560/50000 (5%)]\tLoss: 2.240011\tError: 81.250000\n",
      "Train Epoch: 0.05 [2624/50000 (5%)]\tLoss: 2.315737\tError: 85.937500\n",
      "Train Epoch: 0.05 [2688/50000 (5%)]\tLoss: 2.168191\tError: 78.125000\n",
      "Train Epoch: 0.05 [2752/50000 (5%)]\tLoss: 2.031360\tError: 75.000000\n",
      "Train Epoch: 0.05 [2816/50000 (5%)]\tLoss: 1.936282\tError: 64.062500\n",
      "Train Epoch: 0.06 [2880/50000 (6%)]\tLoss: 2.185359\tError: 76.562500\n",
      "Train Epoch: 0.06 [2944/50000 (6%)]\tLoss: 2.194319\tError: 81.250000\n",
      "Train Epoch: 0.06 [3008/50000 (6%)]\tLoss: 1.953134\tError: 68.750000\n",
      "Train Epoch: 0.06 [3072/50000 (6%)]\tLoss: 2.244652\tError: 78.125000\n",
      "Train Epoch: 0.06 [3136/50000 (6%)]\tLoss: 1.968418\tError: 67.187500\n",
      "Train Epoch: 0.06 [3200/50000 (6%)]\tLoss: 2.083799\tError: 70.312500\n",
      "Train Epoch: 0.06 [3264/50000 (6%)]\tLoss: 2.315881\tError: 78.125000\n",
      "Train Epoch: 0.07 [3328/50000 (7%)]\tLoss: 2.020547\tError: 79.687500\n",
      "Train Epoch: 0.07 [3392/50000 (7%)]\tLoss: 2.092260\tError: 76.562500\n",
      "Train Epoch: 0.07 [3456/50000 (7%)]\tLoss: 2.474948\tError: 87.500000\n",
      "Train Epoch: 0.07 [3520/50000 (7%)]\tLoss: 1.936481\tError: 79.687500\n",
      "Train Epoch: 0.07 [3584/50000 (7%)]\tLoss: 2.018550\tError: 78.125000\n",
      "Train Epoch: 0.07 [3648/50000 (7%)]\tLoss: 2.374889\tError: 79.687500\n",
      "Train Epoch: 0.07 [3712/50000 (7%)]\tLoss: 1.859362\tError: 67.187500\n",
      "Train Epoch: 0.07 [3776/50000 (7%)]\tLoss: 1.955701\tError: 75.000000\n",
      "Train Epoch: 0.08 [3840/50000 (8%)]\tLoss: 2.036666\tError: 71.875000\n",
      "Train Epoch: 0.08 [3904/50000 (8%)]\tLoss: 2.371354\tError: 79.687500\n",
      "Train Epoch: 0.08 [3968/50000 (8%)]\tLoss: 1.802978\tError: 65.625000\n",
      "Train Epoch: 0.08 [4032/50000 (8%)]\tLoss: 1.894599\tError: 73.437500\n",
      "Train Epoch: 0.08 [4096/50000 (8%)]\tLoss: 1.991845\tError: 68.750000\n",
      "Train Epoch: 0.08 [4160/50000 (8%)]\tLoss: 2.000947\tError: 68.750000\n",
      "Train Epoch: 0.08 [4224/50000 (8%)]\tLoss: 1.845846\tError: 73.437500\n",
      "Train Epoch: 0.08 [4288/50000 (8%)]\tLoss: 1.743715\tError: 71.875000\n",
      "Train Epoch: 0.09 [4352/50000 (9%)]\tLoss: 1.881202\tError: 64.062500\n",
      "Train Epoch: 0.09 [4416/50000 (9%)]\tLoss: 2.052196\tError: 73.437500\n",
      "Train Epoch: 0.09 [4480/50000 (9%)]\tLoss: 2.065816\tError: 70.312500\n",
      "Train Epoch: 0.09 [4544/50000 (9%)]\tLoss: 2.028543\tError: 78.125000\n",
      "Train Epoch: 0.09 [4608/50000 (9%)]\tLoss: 2.036891\tError: 79.687500\n",
      "Train Epoch: 0.09 [4672/50000 (9%)]\tLoss: 1.962532\tError: 70.312500\n",
      "Train Epoch: 0.09 [4736/50000 (9%)]\tLoss: 2.083806\tError: 82.812500\n",
      "Train Epoch: 0.09 [4800/50000 (9%)]\tLoss: 2.044255\tError: 75.000000\n",
      "Train Epoch: 0.10 [4864/50000 (10%)]\tLoss: 1.817688\tError: 68.750000\n",
      "Train Epoch: 0.10 [4928/50000 (10%)]\tLoss: 1.731158\tError: 67.187500\n",
      "Train Epoch: 0.10 [4992/50000 (10%)]\tLoss: 2.160963\tError: 84.375000\n",
      "Train Epoch: 0.10 [5056/50000 (10%)]\tLoss: 1.994137\tError: 71.875000\n",
      "Train Epoch: 0.10 [5120/50000 (10%)]\tLoss: 1.761062\tError: 73.437500\n",
      "Train Epoch: 0.10 [5184/50000 (10%)]\tLoss: 1.909470\tError: 68.750000\n",
      "Train Epoch: 0.10 [5248/50000 (10%)]\tLoss: 1.753630\tError: 67.187500\n",
      "Train Epoch: 0.10 [5312/50000 (10%)]\tLoss: 1.742501\tError: 70.312500\n",
      "Train Epoch: 0.11 [5376/50000 (11%)]\tLoss: 1.881636\tError: 73.437500\n",
      "Train Epoch: 0.11 [5440/50000 (11%)]\tLoss: 1.815133\tError: 67.187500\n",
      "Train Epoch: 0.11 [5504/50000 (11%)]\tLoss: 1.714898\tError: 65.625000\n",
      "Train Epoch: 0.11 [5568/50000 (11%)]\tLoss: 1.750461\tError: 73.437500\n",
      "Train Epoch: 0.11 [5632/50000 (11%)]\tLoss: 1.822358\tError: 70.312500\n",
      "Train Epoch: 0.11 [5696/50000 (11%)]\tLoss: 1.737131\tError: 71.875000\n",
      "Train Epoch: 0.11 [5760/50000 (11%)]\tLoss: 1.886036\tError: 67.187500\n",
      "Train Epoch: 0.12 [5824/50000 (12%)]\tLoss: 2.272089\tError: 76.562500\n",
      "Train Epoch: 0.12 [5888/50000 (12%)]\tLoss: 1.706896\tError: 67.187500\n",
      "Train Epoch: 0.12 [5952/50000 (12%)]\tLoss: 1.741142\tError: 60.937500\n",
      "Train Epoch: 0.12 [6016/50000 (12%)]\tLoss: 1.554355\tError: 60.937500\n",
      "Train Epoch: 0.12 [6080/50000 (12%)]\tLoss: 1.691183\tError: 71.875000\n",
      "Train Epoch: 0.12 [6144/50000 (12%)]\tLoss: 2.186507\tError: 75.000000\n",
      "Train Epoch: 0.12 [6208/50000 (12%)]\tLoss: 2.039129\tError: 70.312500\n",
      "Train Epoch: 0.12 [6272/50000 (12%)]\tLoss: 1.742779\tError: 67.187500\n",
      "Train Epoch: 0.13 [6336/50000 (13%)]\tLoss: 1.744690\tError: 73.437500\n",
      "Train Epoch: 0.13 [6400/50000 (13%)]\tLoss: 2.059690\tError: 73.437500\n",
      "Train Epoch: 0.13 [6464/50000 (13%)]\tLoss: 1.821816\tError: 67.187500\n",
      "Train Epoch: 0.13 [6528/50000 (13%)]\tLoss: 1.712651\tError: 62.500000\n",
      "Train Epoch: 0.13 [6592/50000 (13%)]\tLoss: 1.718923\tError: 64.062500\n",
      "Train Epoch: 0.13 [6656/50000 (13%)]\tLoss: 1.717595\tError: 62.500000\n",
      "Train Epoch: 0.13 [6720/50000 (13%)]\tLoss: 1.765248\tError: 57.812500\n",
      "Train Epoch: 0.13 [6784/50000 (13%)]\tLoss: 1.968039\tError: 68.750000\n",
      "Train Epoch: 0.14 [6848/50000 (14%)]\tLoss: 1.667105\tError: 68.750000\n",
      "Train Epoch: 0.14 [6912/50000 (14%)]\tLoss: 1.791703\tError: 71.875000\n",
      "Train Epoch: 0.14 [6976/50000 (14%)]\tLoss: 1.862479\tError: 67.187500\n",
      "Train Epoch: 0.14 [7040/50000 (14%)]\tLoss: 1.902685\tError: 62.500000\n",
      "Train Epoch: 0.14 [7104/50000 (14%)]\tLoss: 1.801311\tError: 64.062500\n",
      "Train Epoch: 0.14 [7168/50000 (14%)]\tLoss: 1.887608\tError: 76.562500\n",
      "Train Epoch: 0.14 [7232/50000 (14%)]\tLoss: 1.955047\tError: 73.437500\n",
      "Train Epoch: 0.14 [7296/50000 (14%)]\tLoss: 1.615136\tError: 67.187500\n",
      "Train Epoch: 0.15 [7360/50000 (15%)]\tLoss: 1.902367\tError: 71.875000\n",
      "Train Epoch: 0.15 [7424/50000 (15%)]\tLoss: 1.919584\tError: 67.187500\n",
      "Train Epoch: 0.15 [7488/50000 (15%)]\tLoss: 1.851879\tError: 75.000000\n",
      "Train Epoch: 0.15 [7552/50000 (15%)]\tLoss: 1.668333\tError: 67.187500\n",
      "Train Epoch: 0.15 [7616/50000 (15%)]\tLoss: 1.914230\tError: 78.125000\n",
      "Train Epoch: 0.15 [7680/50000 (15%)]\tLoss: 2.027177\tError: 75.000000\n",
      "Train Epoch: 0.15 [7744/50000 (15%)]\tLoss: 1.795892\tError: 62.500000\n",
      "Train Epoch: 0.15 [7808/50000 (15%)]\tLoss: 1.806166\tError: 62.500000\n",
      "Train Epoch: 0.16 [7872/50000 (16%)]\tLoss: 1.938638\tError: 71.875000\n",
      "Train Epoch: 0.16 [7936/50000 (16%)]\tLoss: 1.810056\tError: 65.625000\n",
      "Train Epoch: 0.16 [8000/50000 (16%)]\tLoss: 1.813297\tError: 68.750000\n",
      "Train Epoch: 0.16 [8064/50000 (16%)]\tLoss: 1.853471\tError: 70.312500\n",
      "Train Epoch: 0.16 [8128/50000 (16%)]\tLoss: 1.753691\tError: 62.500000\n",
      "Train Epoch: 0.16 [8192/50000 (16%)]\tLoss: 1.811028\tError: 70.312500\n",
      "Train Epoch: 0.16 [8256/50000 (16%)]\tLoss: 1.761210\tError: 67.187500\n",
      "Train Epoch: 0.16 [8320/50000 (16%)]\tLoss: 1.770711\tError: 70.312500\n",
      "Train Epoch: 0.17 [8384/50000 (17%)]\tLoss: 1.918911\tError: 68.750000\n",
      "Train Epoch: 0.17 [8448/50000 (17%)]\tLoss: 1.749202\tError: 57.812500\n",
      "Train Epoch: 0.17 [8512/50000 (17%)]\tLoss: 1.825279\tError: 65.625000\n",
      "Train Epoch: 0.17 [8576/50000 (17%)]\tLoss: 1.874549\tError: 75.000000\n",
      "Train Epoch: 0.17 [8640/50000 (17%)]\tLoss: 1.700714\tError: 70.312500\n",
      "Train Epoch: 0.17 [8704/50000 (17%)]\tLoss: 1.774632\tError: 64.062500\n",
      "Train Epoch: 0.17 [8768/50000 (17%)]\tLoss: 1.601315\tError: 56.250000\n",
      "Train Epoch: 0.18 [8832/50000 (18%)]\tLoss: 1.772104\tError: 70.312500\n",
      "Train Epoch: 0.18 [8896/50000 (18%)]\tLoss: 1.942969\tError: 73.437500\n",
      "Train Epoch: 0.18 [8960/50000 (18%)]\tLoss: 1.902394\tError: 70.312500\n",
      "Train Epoch: 0.18 [9024/50000 (18%)]\tLoss: 2.029549\tError: 73.437500\n",
      "Train Epoch: 0.18 [9088/50000 (18%)]\tLoss: 1.977398\tError: 76.562500\n",
      "Train Epoch: 0.18 [9152/50000 (18%)]\tLoss: 1.708192\tError: 64.062500\n",
      "Train Epoch: 0.18 [9216/50000 (18%)]\tLoss: 1.815779\tError: 67.187500\n",
      "Train Epoch: 0.18 [9280/50000 (18%)]\tLoss: 1.594848\tError: 60.937500\n",
      "Train Epoch: 0.19 [9344/50000 (19%)]\tLoss: 1.830832\tError: 67.187500\n",
      "Train Epoch: 0.19 [9408/50000 (19%)]\tLoss: 1.720331\tError: 60.937500\n",
      "Train Epoch: 0.19 [9472/50000 (19%)]\tLoss: 1.653635\tError: 64.062500\n",
      "Train Epoch: 0.19 [9536/50000 (19%)]\tLoss: 1.635314\tError: 56.250000\n",
      "Train Epoch: 0.19 [9600/50000 (19%)]\tLoss: 1.609891\tError: 65.625000\n",
      "Train Epoch: 0.19 [9664/50000 (19%)]\tLoss: 1.799859\tError: 62.500000\n",
      "Train Epoch: 0.19 [9728/50000 (19%)]\tLoss: 1.767985\tError: 68.750000\n",
      "Train Epoch: 0.19 [9792/50000 (19%)]\tLoss: 1.714217\tError: 65.625000\n",
      "Train Epoch: 0.20 [9856/50000 (20%)]\tLoss: 1.989585\tError: 70.312500\n",
      "Train Epoch: 0.20 [9920/50000 (20%)]\tLoss: 1.882558\tError: 68.750000\n",
      "Train Epoch: 0.20 [9984/50000 (20%)]\tLoss: 1.979243\tError: 73.437500\n",
      "Train Epoch: 0.20 [10048/50000 (20%)]\tLoss: 2.015244\tError: 71.875000\n",
      "Train Epoch: 0.20 [10112/50000 (20%)]\tLoss: 1.649561\tError: 59.375000\n",
      "Train Epoch: 0.20 [10176/50000 (20%)]\tLoss: 1.784153\tError: 67.187500\n",
      "Train Epoch: 0.20 [10240/50000 (20%)]\tLoss: 1.745976\tError: 56.250000\n",
      "Train Epoch: 0.20 [10304/50000 (20%)]\tLoss: 2.067297\tError: 70.312500\n",
      "Train Epoch: 0.21 [10368/50000 (21%)]\tLoss: 1.889024\tError: 62.500000\n",
      "Train Epoch: 0.21 [10432/50000 (21%)]\tLoss: 1.951923\tError: 68.750000\n",
      "Train Epoch: 0.21 [10496/50000 (21%)]\tLoss: 1.746614\tError: 62.500000\n",
      "Train Epoch: 0.21 [10560/50000 (21%)]\tLoss: 1.904733\tError: 78.125000\n",
      "Train Epoch: 0.21 [10624/50000 (21%)]\tLoss: 2.055192\tError: 73.437500\n",
      "Train Epoch: 0.21 [10688/50000 (21%)]\tLoss: 1.737253\tError: 53.125000\n",
      "Train Epoch: 0.21 [10752/50000 (21%)]\tLoss: 2.027048\tError: 68.750000\n",
      "Train Epoch: 0.21 [10816/50000 (21%)]\tLoss: 1.562939\tError: 57.812500\n",
      "Train Epoch: 0.22 [10880/50000 (22%)]\tLoss: 1.508607\tError: 60.937500\n",
      "Train Epoch: 0.22 [10944/50000 (22%)]\tLoss: 1.915266\tError: 68.750000\n",
      "Train Epoch: 0.22 [11008/50000 (22%)]\tLoss: 1.636282\tError: 56.250000\n",
      "Train Epoch: 0.22 [11072/50000 (22%)]\tLoss: 1.859674\tError: 60.937500\n",
      "Train Epoch: 0.22 [11136/50000 (22%)]\tLoss: 1.469385\tError: 65.625000\n",
      "Train Epoch: 0.22 [11200/50000 (22%)]\tLoss: 1.646871\tError: 56.250000\n",
      "Train Epoch: 0.22 [11264/50000 (22%)]\tLoss: 1.587316\tError: 68.750000\n",
      "Train Epoch: 0.23 [11328/50000 (23%)]\tLoss: 1.820748\tError: 75.000000\n",
      "Train Epoch: 0.23 [11392/50000 (23%)]\tLoss: 1.644917\tError: 54.687500\n",
      "Train Epoch: 0.23 [11456/50000 (23%)]\tLoss: 1.819631\tError: 67.187500\n",
      "Train Epoch: 0.23 [11520/50000 (23%)]\tLoss: 1.922845\tError: 68.750000\n",
      "Train Epoch: 0.23 [11584/50000 (23%)]\tLoss: 1.722468\tError: 57.812500\n",
      "Train Epoch: 0.23 [11648/50000 (23%)]\tLoss: 1.781241\tError: 67.187500\n",
      "Train Epoch: 0.23 [11712/50000 (23%)]\tLoss: 1.682020\tError: 67.187500\n",
      "Train Epoch: 0.23 [11776/50000 (23%)]\tLoss: 1.862737\tError: 67.187500\n",
      "Train Epoch: 0.24 [11840/50000 (24%)]\tLoss: 1.888114\tError: 70.312500\n",
      "Train Epoch: 0.24 [11904/50000 (24%)]\tLoss: 1.787616\tError: 67.187500\n",
      "Train Epoch: 0.24 [11968/50000 (24%)]\tLoss: 1.858505\tError: 65.625000\n",
      "Train Epoch: 0.24 [12032/50000 (24%)]\tLoss: 1.708071\tError: 60.937500\n",
      "Train Epoch: 0.24 [12096/50000 (24%)]\tLoss: 1.800738\tError: 67.187500\n",
      "Train Epoch: 0.24 [12160/50000 (24%)]\tLoss: 1.754771\tError: 71.875000\n",
      "Train Epoch: 0.24 [12224/50000 (24%)]\tLoss: 1.468078\tError: 59.375000\n",
      "Train Epoch: 0.24 [12288/50000 (24%)]\tLoss: 1.710775\tError: 65.625000\n",
      "Train Epoch: 0.25 [12352/50000 (25%)]\tLoss: 1.512910\tError: 54.687500\n",
      "Train Epoch: 0.25 [12416/50000 (25%)]\tLoss: 1.772910\tError: 65.625000\n",
      "Train Epoch: 0.25 [12480/50000 (25%)]\tLoss: 1.516938\tError: 57.812500\n",
      "Train Epoch: 0.25 [12544/50000 (25%)]\tLoss: 1.882797\tError: 65.625000\n",
      "Train Epoch: 0.25 [12608/50000 (25%)]\tLoss: 1.958124\tError: 71.875000\n",
      "Train Epoch: 0.25 [12672/50000 (25%)]\tLoss: 1.794872\tError: 67.187500\n",
      "Train Epoch: 0.25 [12736/50000 (25%)]\tLoss: 1.628534\tError: 59.375000\n",
      "Train Epoch: 0.25 [12800/50000 (25%)]\tLoss: 1.727882\tError: 73.437500\n",
      "Train Epoch: 0.26 [12864/50000 (26%)]\tLoss: 1.730998\tError: 62.500000\n",
      "Train Epoch: 0.26 [12928/50000 (26%)]\tLoss: 1.457780\tError: 56.250000\n",
      "Train Epoch: 0.26 [12992/50000 (26%)]\tLoss: 1.478374\tError: 53.125000\n",
      "Train Epoch: 0.26 [13056/50000 (26%)]\tLoss: 1.496490\tError: 59.375000\n",
      "Train Epoch: 0.26 [13120/50000 (26%)]\tLoss: 1.657029\tError: 62.500000\n",
      "Train Epoch: 0.26 [13184/50000 (26%)]\tLoss: 1.816808\tError: 67.187500\n",
      "Train Epoch: 0.26 [13248/50000 (26%)]\tLoss: 1.478104\tError: 57.812500\n",
      "Train Epoch: 0.26 [13312/50000 (26%)]\tLoss: 1.898837\tError: 62.500000\n",
      "Train Epoch: 0.27 [13376/50000 (27%)]\tLoss: 2.102361\tError: 73.437500\n",
      "Train Epoch: 0.27 [13440/50000 (27%)]\tLoss: 1.502940\tError: 54.687500\n",
      "Train Epoch: 0.27 [13504/50000 (27%)]\tLoss: 1.661258\tError: 59.375000\n",
      "Train Epoch: 0.27 [13568/50000 (27%)]\tLoss: 1.668180\tError: 57.812500\n",
      "Train Epoch: 0.27 [13632/50000 (27%)]\tLoss: 1.738362\tError: 67.187500\n",
      "Train Epoch: 0.27 [13696/50000 (27%)]\tLoss: 1.603951\tError: 65.625000\n",
      "Train Epoch: 0.27 [13760/50000 (27%)]\tLoss: 1.699584\tError: 70.312500\n",
      "Train Epoch: 0.27 [13824/50000 (27%)]\tLoss: 1.613791\tError: 53.125000\n",
      "Train Epoch: 0.28 [13888/50000 (28%)]\tLoss: 1.748349\tError: 62.500000\n",
      "Train Epoch: 0.28 [13952/50000 (28%)]\tLoss: 1.776143\tError: 62.500000\n",
      "Train Epoch: 0.28 [14016/50000 (28%)]\tLoss: 1.586517\tError: 60.937500\n",
      "Train Epoch: 0.28 [14080/50000 (28%)]\tLoss: 1.651656\tError: 54.687500\n",
      "Train Epoch: 0.28 [14144/50000 (28%)]\tLoss: 1.629194\tError: 57.812500\n",
      "Train Epoch: 0.28 [14208/50000 (28%)]\tLoss: 1.669529\tError: 54.687500\n",
      "Train Epoch: 0.28 [14272/50000 (28%)]\tLoss: 1.924327\tError: 60.937500\n",
      "Train Epoch: 0.29 [14336/50000 (29%)]\tLoss: 1.981164\tError: 67.187500\n",
      "Train Epoch: 0.29 [14400/50000 (29%)]\tLoss: 1.985294\tError: 71.875000\n",
      "Train Epoch: 0.29 [14464/50000 (29%)]\tLoss: 1.438562\tError: 57.812500\n",
      "Train Epoch: 0.29 [14528/50000 (29%)]\tLoss: 1.643762\tError: 60.937500\n",
      "Train Epoch: 0.29 [14592/50000 (29%)]\tLoss: 1.626162\tError: 62.500000\n",
      "Train Epoch: 0.29 [14656/50000 (29%)]\tLoss: 1.747324\tError: 71.875000\n",
      "Train Epoch: 0.29 [14720/50000 (29%)]\tLoss: 1.817849\tError: 71.875000\n",
      "Train Epoch: 0.29 [14784/50000 (29%)]\tLoss: 1.441253\tError: 56.250000\n",
      "Train Epoch: 0.30 [14848/50000 (30%)]\tLoss: 1.543517\tError: 59.375000\n",
      "Train Epoch: 0.30 [14912/50000 (30%)]\tLoss: 1.517744\tError: 59.375000\n",
      "Train Epoch: 0.30 [14976/50000 (30%)]\tLoss: 1.574107\tError: 59.375000\n",
      "Train Epoch: 0.30 [15040/50000 (30%)]\tLoss: 1.982742\tError: 67.187500\n",
      "Train Epoch: 0.30 [15104/50000 (30%)]\tLoss: 1.444643\tError: 53.125000\n",
      "Train Epoch: 0.30 [15168/50000 (30%)]\tLoss: 1.498786\tError: 56.250000\n",
      "Train Epoch: 0.30 [15232/50000 (30%)]\tLoss: 1.712312\tError: 60.937500\n",
      "Train Epoch: 0.30 [15296/50000 (30%)]\tLoss: 1.642288\tError: 68.750000\n",
      "Train Epoch: 0.31 [15360/50000 (31%)]\tLoss: 1.697528\tError: 59.375000\n",
      "Train Epoch: 0.31 [15424/50000 (31%)]\tLoss: 1.842616\tError: 59.375000\n",
      "Train Epoch: 0.31 [15488/50000 (31%)]\tLoss: 1.807950\tError: 60.937500\n",
      "Train Epoch: 0.31 [15552/50000 (31%)]\tLoss: 1.640591\tError: 60.937500\n",
      "Train Epoch: 0.31 [15616/50000 (31%)]\tLoss: 1.671254\tError: 59.375000\n",
      "Train Epoch: 0.31 [15680/50000 (31%)]\tLoss: 1.487328\tError: 50.000000\n",
      "Train Epoch: 0.31 [15744/50000 (31%)]\tLoss: 1.616180\tError: 65.625000\n",
      "Train Epoch: 0.31 [15808/50000 (31%)]\tLoss: 1.563321\tError: 48.437500\n",
      "Train Epoch: 0.32 [15872/50000 (32%)]\tLoss: 1.609286\tError: 62.500000\n",
      "Train Epoch: 0.32 [15936/50000 (32%)]\tLoss: 1.653256\tError: 60.937500\n",
      "Train Epoch: 0.32 [16000/50000 (32%)]\tLoss: 1.460683\tError: 60.937500\n",
      "Train Epoch: 0.32 [16064/50000 (32%)]\tLoss: 1.624092\tError: 57.812500\n",
      "Train Epoch: 0.32 [16128/50000 (32%)]\tLoss: 1.521858\tError: 56.250000\n",
      "Train Epoch: 0.32 [16192/50000 (32%)]\tLoss: 1.827382\tError: 62.500000\n",
      "Train Epoch: 0.32 [16256/50000 (32%)]\tLoss: 1.445549\tError: 56.250000\n",
      "Train Epoch: 0.32 [16320/50000 (32%)]\tLoss: 1.736560\tError: 59.375000\n",
      "Train Epoch: 0.33 [16384/50000 (33%)]\tLoss: 1.391479\tError: 56.250000\n",
      "Train Epoch: 0.33 [16448/50000 (33%)]\tLoss: 1.438410\tError: 51.562500\n",
      "Train Epoch: 0.33 [16512/50000 (33%)]\tLoss: 1.737868\tError: 65.625000\n",
      "Train Epoch: 0.33 [16576/50000 (33%)]\tLoss: 1.704706\tError: 59.375000\n",
      "Train Epoch: 0.33 [16640/50000 (33%)]\tLoss: 1.489549\tError: 51.562500\n",
      "Train Epoch: 0.33 [16704/50000 (33%)]\tLoss: 1.594811\tError: 54.687500\n",
      "Train Epoch: 0.33 [16768/50000 (33%)]\tLoss: 1.582397\tError: 59.375000\n",
      "Train Epoch: 0.34 [16832/50000 (34%)]\tLoss: 1.689936\tError: 57.812500\n",
      "Train Epoch: 0.34 [16896/50000 (34%)]\tLoss: 1.565549\tError: 60.937500\n",
      "Train Epoch: 0.34 [16960/50000 (34%)]\tLoss: 1.461831\tError: 62.500000\n",
      "Train Epoch: 0.34 [17024/50000 (34%)]\tLoss: 1.499699\tError: 57.812500\n",
      "Train Epoch: 0.34 [17088/50000 (34%)]\tLoss: 1.614552\tError: 51.562500\n",
      "Train Epoch: 0.34 [17152/50000 (34%)]\tLoss: 1.514722\tError: 56.250000\n",
      "Train Epoch: 0.34 [17216/50000 (34%)]\tLoss: 1.340165\tError: 43.750000\n",
      "Train Epoch: 0.34 [17280/50000 (34%)]\tLoss: 1.517900\tError: 50.000000\n",
      "Train Epoch: 0.35 [17344/50000 (35%)]\tLoss: 1.561260\tError: 54.687500\n",
      "Train Epoch: 0.35 [17408/50000 (35%)]\tLoss: 1.299882\tError: 50.000000\n",
      "Train Epoch: 0.35 [17472/50000 (35%)]\tLoss: 1.582859\tError: 59.375000\n",
      "Train Epoch: 0.35 [17536/50000 (35%)]\tLoss: 1.723473\tError: 62.500000\n",
      "Train Epoch: 0.35 [17600/50000 (35%)]\tLoss: 1.468550\tError: 59.375000\n",
      "Train Epoch: 0.35 [17664/50000 (35%)]\tLoss: 1.695786\tError: 62.500000\n",
      "Train Epoch: 0.35 [17728/50000 (35%)]\tLoss: 1.461927\tError: 45.312500\n",
      "Train Epoch: 0.35 [17792/50000 (35%)]\tLoss: 1.567667\tError: 60.937500\n",
      "Train Epoch: 0.36 [17856/50000 (36%)]\tLoss: 1.670122\tError: 64.062500\n",
      "Train Epoch: 0.36 [17920/50000 (36%)]\tLoss: 1.509210\tError: 53.125000\n",
      "Train Epoch: 0.36 [17984/50000 (36%)]\tLoss: 1.431506\tError: 56.250000\n",
      "Train Epoch: 0.36 [18048/50000 (36%)]\tLoss: 1.625150\tError: 57.812500\n",
      "Train Epoch: 0.36 [18112/50000 (36%)]\tLoss: 1.493876\tError: 50.000000\n",
      "Train Epoch: 0.36 [18176/50000 (36%)]\tLoss: 1.371334\tError: 51.562500\n",
      "Train Epoch: 0.36 [18240/50000 (36%)]\tLoss: 1.455605\tError: 62.500000\n",
      "Train Epoch: 0.36 [18304/50000 (36%)]\tLoss: 1.243151\tError: 46.875000\n",
      "Train Epoch: 0.37 [18368/50000 (37%)]\tLoss: 1.527245\tError: 51.562500\n",
      "Train Epoch: 0.37 [18432/50000 (37%)]\tLoss: 1.069492\tError: 39.062500\n",
      "Train Epoch: 0.37 [18496/50000 (37%)]\tLoss: 1.416611\tError: 50.000000\n",
      "Train Epoch: 0.37 [18560/50000 (37%)]\tLoss: 1.530263\tError: 53.125000\n",
      "Train Epoch: 0.37 [18624/50000 (37%)]\tLoss: 1.312461\tError: 43.750000\n",
      "Train Epoch: 0.37 [18688/50000 (37%)]\tLoss: 1.499476\tError: 50.000000\n",
      "Train Epoch: 0.37 [18752/50000 (37%)]\tLoss: 1.430987\tError: 54.687500\n",
      "Train Epoch: 0.37 [18816/50000 (37%)]\tLoss: 1.463369\tError: 54.687500\n",
      "Train Epoch: 0.38 [18880/50000 (38%)]\tLoss: 1.686354\tError: 53.125000\n",
      "Train Epoch: 0.38 [18944/50000 (38%)]\tLoss: 1.308756\tError: 46.875000\n",
      "Train Epoch: 0.38 [19008/50000 (38%)]\tLoss: 1.377940\tError: 53.125000\n",
      "Train Epoch: 0.38 [19072/50000 (38%)]\tLoss: 1.523252\tError: 60.937500\n",
      "Train Epoch: 0.38 [19136/50000 (38%)]\tLoss: 1.315329\tError: 42.187500\n",
      "Train Epoch: 0.38 [19200/50000 (38%)]\tLoss: 1.497238\tError: 57.812500\n",
      "Train Epoch: 0.38 [19264/50000 (38%)]\tLoss: 1.384176\tError: 51.562500\n",
      "Train Epoch: 0.38 [19328/50000 (38%)]\tLoss: 1.537277\tError: 51.562500\n",
      "Train Epoch: 0.39 [19392/50000 (39%)]\tLoss: 1.425543\tError: 50.000000\n",
      "Train Epoch: 0.39 [19456/50000 (39%)]\tLoss: 1.386083\tError: 54.687500\n",
      "Train Epoch: 0.39 [19520/50000 (39%)]\tLoss: 1.196443\tError: 48.437500\n",
      "Train Epoch: 0.39 [19584/50000 (39%)]\tLoss: 1.396707\tError: 54.687500\n",
      "Train Epoch: 0.39 [19648/50000 (39%)]\tLoss: 1.411537\tError: 59.375000\n",
      "Train Epoch: 0.39 [19712/50000 (39%)]\tLoss: 1.637195\tError: 54.687500\n",
      "Train Epoch: 0.39 [19776/50000 (39%)]\tLoss: 1.308542\tError: 53.125000\n",
      "Train Epoch: 0.40 [19840/50000 (40%)]\tLoss: 1.617363\tError: 54.687500\n",
      "Train Epoch: 0.40 [19904/50000 (40%)]\tLoss: 1.319974\tError: 50.000000\n",
      "Train Epoch: 0.40 [19968/50000 (40%)]\tLoss: 1.243596\tError: 40.625000\n",
      "Train Epoch: 0.40 [20032/50000 (40%)]\tLoss: 1.557885\tError: 60.937500\n",
      "Train Epoch: 0.40 [20096/50000 (40%)]\tLoss: 1.211214\tError: 43.750000\n",
      "Train Epoch: 0.40 [20160/50000 (40%)]\tLoss: 1.577052\tError: 57.812500\n",
      "Train Epoch: 0.40 [20224/50000 (40%)]\tLoss: 1.807465\tError: 71.875000\n",
      "Train Epoch: 0.40 [20288/50000 (40%)]\tLoss: 1.424782\tError: 50.000000\n",
      "Train Epoch: 0.41 [20352/50000 (41%)]\tLoss: 1.629241\tError: 57.812500\n",
      "Train Epoch: 0.41 [20416/50000 (41%)]\tLoss: 1.537265\tError: 51.562500\n",
      "Train Epoch: 0.41 [20480/50000 (41%)]\tLoss: 1.689760\tError: 64.062500\n",
      "Train Epoch: 0.41 [20544/50000 (41%)]\tLoss: 1.626680\tError: 57.812500\n",
      "Train Epoch: 0.41 [20608/50000 (41%)]\tLoss: 1.472675\tError: 59.375000\n",
      "Train Epoch: 0.41 [20672/50000 (41%)]\tLoss: 1.424249\tError: 56.250000\n",
      "Train Epoch: 0.41 [20736/50000 (41%)]\tLoss: 1.579278\tError: 57.812500\n",
      "Train Epoch: 0.41 [20800/50000 (41%)]\tLoss: 1.525031\tError: 64.062500\n",
      "Train Epoch: 0.42 [20864/50000 (42%)]\tLoss: 1.531352\tError: 45.312500\n",
      "Train Epoch: 0.42 [20928/50000 (42%)]\tLoss: 1.351138\tError: 53.125000\n",
      "Train Epoch: 0.42 [20992/50000 (42%)]\tLoss: 1.224713\tError: 50.000000\n",
      "Train Epoch: 0.42 [21056/50000 (42%)]\tLoss: 1.496436\tError: 59.375000\n",
      "Train Epoch: 0.42 [21120/50000 (42%)]\tLoss: 1.400807\tError: 51.562500\n",
      "Train Epoch: 0.42 [21184/50000 (42%)]\tLoss: 1.382799\tError: 54.687500\n",
      "Train Epoch: 0.42 [21248/50000 (42%)]\tLoss: 1.557963\tError: 53.125000\n",
      "Train Epoch: 0.42 [21312/50000 (42%)]\tLoss: 1.408726\tError: 56.250000\n",
      "Train Epoch: 0.43 [21376/50000 (43%)]\tLoss: 1.270792\tError: 46.875000\n",
      "Train Epoch: 0.43 [21440/50000 (43%)]\tLoss: 1.510900\tError: 53.125000\n",
      "Train Epoch: 0.43 [21504/50000 (43%)]\tLoss: 1.717709\tError: 60.937500\n",
      "Train Epoch: 0.43 [21568/50000 (43%)]\tLoss: 1.224155\tError: 37.500000\n",
      "Train Epoch: 0.43 [21632/50000 (43%)]\tLoss: 1.517156\tError: 56.250000\n",
      "Train Epoch: 0.43 [21696/50000 (43%)]\tLoss: 1.423887\tError: 48.437500\n",
      "Train Epoch: 0.43 [21760/50000 (43%)]\tLoss: 1.621342\tError: 56.250000\n",
      "Train Epoch: 0.43 [21824/50000 (43%)]\tLoss: 1.542058\tError: 51.562500\n",
      "Train Epoch: 0.44 [21888/50000 (44%)]\tLoss: 1.442641\tError: 43.750000\n",
      "Train Epoch: 0.44 [21952/50000 (44%)]\tLoss: 1.541378\tError: 51.562500\n",
      "Train Epoch: 0.44 [22016/50000 (44%)]\tLoss: 1.556296\tError: 62.500000\n",
      "Train Epoch: 0.44 [22080/50000 (44%)]\tLoss: 1.435742\tError: 59.375000\n",
      "Train Epoch: 0.44 [22144/50000 (44%)]\tLoss: 1.602231\tError: 59.375000\n",
      "Train Epoch: 0.44 [22208/50000 (44%)]\tLoss: 1.669821\tError: 57.812500\n",
      "Train Epoch: 0.44 [22272/50000 (44%)]\tLoss: 1.583679\tError: 60.937500\n",
      "Train Epoch: 0.45 [22336/50000 (45%)]\tLoss: 1.277602\tError: 48.437500\n",
      "Train Epoch: 0.45 [22400/50000 (45%)]\tLoss: 1.162630\tError: 45.312500\n",
      "Train Epoch: 0.45 [22464/50000 (45%)]\tLoss: 1.431379\tError: 57.812500\n",
      "Train Epoch: 0.45 [22528/50000 (45%)]\tLoss: 1.143454\tError: 40.625000\n",
      "Train Epoch: 0.45 [22592/50000 (45%)]\tLoss: 1.289420\tError: 48.437500\n",
      "Train Epoch: 0.45 [22656/50000 (45%)]\tLoss: 1.475455\tError: 53.125000\n",
      "Train Epoch: 0.45 [22720/50000 (45%)]\tLoss: 1.508169\tError: 59.375000\n",
      "Train Epoch: 0.45 [22784/50000 (45%)]\tLoss: 1.600062\tError: 60.937500\n",
      "Train Epoch: 0.46 [22848/50000 (46%)]\tLoss: 1.334369\tError: 50.000000\n",
      "Train Epoch: 0.46 [22912/50000 (46%)]\tLoss: 1.526338\tError: 60.937500\n",
      "Train Epoch: 0.46 [22976/50000 (46%)]\tLoss: 1.481571\tError: 53.125000\n",
      "Train Epoch: 0.46 [23040/50000 (46%)]\tLoss: 1.522366\tError: 51.562500\n",
      "Train Epoch: 0.46 [23104/50000 (46%)]\tLoss: 1.328011\tError: 45.312500\n",
      "Train Epoch: 0.46 [23168/50000 (46%)]\tLoss: 1.556732\tError: 60.937500\n",
      "Train Epoch: 0.46 [23232/50000 (46%)]\tLoss: 1.408178\tError: 46.875000\n",
      "Train Epoch: 0.46 [23296/50000 (46%)]\tLoss: 1.586809\tError: 59.375000\n",
      "Train Epoch: 0.47 [23360/50000 (47%)]\tLoss: 1.605147\tError: 57.812500\n",
      "Train Epoch: 0.47 [23424/50000 (47%)]\tLoss: 1.139637\tError: 43.750000\n",
      "Train Epoch: 0.47 [23488/50000 (47%)]\tLoss: 1.506300\tError: 53.125000\n",
      "Train Epoch: 0.47 [23552/50000 (47%)]\tLoss: 1.323572\tError: 45.312500\n",
      "Train Epoch: 0.47 [23616/50000 (47%)]\tLoss: 1.396714\tError: 51.562500\n",
      "Train Epoch: 0.47 [23680/50000 (47%)]\tLoss: 1.527311\tError: 53.125000\n",
      "Train Epoch: 0.47 [23744/50000 (47%)]\tLoss: 1.644145\tError: 56.250000\n",
      "Train Epoch: 0.47 [23808/50000 (47%)]\tLoss: 1.878491\tError: 73.437500\n",
      "Train Epoch: 0.48 [23872/50000 (48%)]\tLoss: 1.451721\tError: 53.125000\n",
      "Train Epoch: 0.48 [23936/50000 (48%)]\tLoss: 1.596377\tError: 57.812500\n",
      "Train Epoch: 0.48 [24000/50000 (48%)]\tLoss: 1.206981\tError: 43.750000\n",
      "Train Epoch: 0.48 [24064/50000 (48%)]\tLoss: 1.167587\tError: 45.312500\n",
      "Train Epoch: 0.48 [24128/50000 (48%)]\tLoss: 1.532868\tError: 59.375000\n",
      "Train Epoch: 0.48 [24192/50000 (48%)]\tLoss: 1.396516\tError: 51.562500\n",
      "Train Epoch: 0.48 [24256/50000 (48%)]\tLoss: 1.531842\tError: 54.687500\n",
      "Train Epoch: 0.48 [24320/50000 (48%)]\tLoss: 1.482799\tError: 57.812500\n",
      "Train Epoch: 0.49 [24384/50000 (49%)]\tLoss: 1.280852\tError: 40.625000\n",
      "Train Epoch: 0.49 [24448/50000 (49%)]\tLoss: 1.711353\tError: 56.250000\n",
      "Train Epoch: 0.49 [24512/50000 (49%)]\tLoss: 1.263795\tError: 50.000000\n",
      "Train Epoch: 0.49 [24576/50000 (49%)]\tLoss: 1.232797\tError: 56.250000\n",
      "Train Epoch: 0.49 [24640/50000 (49%)]\tLoss: 1.479486\tError: 54.687500\n",
      "Train Epoch: 0.49 [24704/50000 (49%)]\tLoss: 1.470814\tError: 46.875000\n",
      "Train Epoch: 0.49 [24768/50000 (49%)]\tLoss: 1.452672\tError: 59.375000\n",
      "Train Epoch: 0.49 [24832/50000 (49%)]\tLoss: 1.578909\tError: 48.437500\n",
      "Train Epoch: 0.50 [24896/50000 (50%)]\tLoss: 1.303336\tError: 50.000000\n",
      "Train Epoch: 0.50 [24960/50000 (50%)]\tLoss: 1.523541\tError: 56.250000\n",
      "Train Epoch: 0.50 [25024/50000 (50%)]\tLoss: 1.512725\tError: 48.437500\n",
      "Train Epoch: 0.50 [25088/50000 (50%)]\tLoss: 1.660000\tError: 57.812500\n",
      "Train Epoch: 0.50 [25152/50000 (50%)]\tLoss: 1.375595\tError: 54.687500\n",
      "Train Epoch: 0.50 [25216/50000 (50%)]\tLoss: 1.310247\tError: 57.812500\n",
      "Train Epoch: 0.50 [25280/50000 (50%)]\tLoss: 1.408990\tError: 53.125000\n",
      "Train Epoch: 0.51 [25344/50000 (51%)]\tLoss: 1.442485\tError: 50.000000\n",
      "Train Epoch: 0.51 [25408/50000 (51%)]\tLoss: 1.373851\tError: 46.875000\n",
      "Train Epoch: 0.51 [25472/50000 (51%)]\tLoss: 1.394680\tError: 46.875000\n",
      "Train Epoch: 0.51 [25536/50000 (51%)]\tLoss: 1.586149\tError: 67.187500\n",
      "Train Epoch: 0.51 [25600/50000 (51%)]\tLoss: 1.151462\tError: 32.812500\n",
      "Train Epoch: 0.51 [25664/50000 (51%)]\tLoss: 1.387194\tError: 59.375000\n",
      "Train Epoch: 0.51 [25728/50000 (51%)]\tLoss: 1.495548\tError: 51.562500\n",
      "Train Epoch: 0.51 [25792/50000 (51%)]\tLoss: 1.494287\tError: 48.437500\n",
      "Train Epoch: 0.52 [25856/50000 (52%)]\tLoss: 1.234273\tError: 50.000000\n",
      "Train Epoch: 0.52 [25920/50000 (52%)]\tLoss: 1.101865\tError: 42.187500\n",
      "Train Epoch: 0.52 [25984/50000 (52%)]\tLoss: 1.199780\tError: 43.750000\n",
      "Train Epoch: 0.52 [26048/50000 (52%)]\tLoss: 1.110721\tError: 40.625000\n",
      "Train Epoch: 0.52 [26112/50000 (52%)]\tLoss: 1.253577\tError: 45.312500\n",
      "Train Epoch: 0.52 [26176/50000 (52%)]\tLoss: 1.116168\tError: 42.187500\n",
      "Train Epoch: 0.52 [26240/50000 (52%)]\tLoss: 1.409398\tError: 53.125000\n",
      "Train Epoch: 0.52 [26304/50000 (52%)]\tLoss: 1.632852\tError: 57.812500\n",
      "Train Epoch: 0.53 [26368/50000 (53%)]\tLoss: 1.283171\tError: 42.187500\n",
      "Train Epoch: 0.53 [26432/50000 (53%)]\tLoss: 1.401196\tError: 53.125000\n",
      "Train Epoch: 0.53 [26496/50000 (53%)]\tLoss: 1.163478\tError: 43.750000\n",
      "Train Epoch: 0.53 [26560/50000 (53%)]\tLoss: 1.367729\tError: 50.000000\n",
      "Train Epoch: 0.53 [26624/50000 (53%)]\tLoss: 1.325690\tError: 48.437500\n",
      "Train Epoch: 0.53 [26688/50000 (53%)]\tLoss: 1.148567\tError: 40.625000\n",
      "Train Epoch: 0.53 [26752/50000 (53%)]\tLoss: 1.329928\tError: 46.875000\n",
      "Train Epoch: 0.53 [26816/50000 (53%)]\tLoss: 1.250774\tError: 46.875000\n",
      "Train Epoch: 0.54 [26880/50000 (54%)]\tLoss: 1.219755\tError: 50.000000\n",
      "Train Epoch: 0.54 [26944/50000 (54%)]\tLoss: 1.566725\tError: 59.375000\n",
      "Train Epoch: 0.54 [27008/50000 (54%)]\tLoss: 1.170076\tError: 40.625000\n",
      "Train Epoch: 0.54 [27072/50000 (54%)]\tLoss: 1.194771\tError: 40.625000\n",
      "Train Epoch: 0.54 [27136/50000 (54%)]\tLoss: 1.412615\tError: 50.000000\n",
      "Train Epoch: 0.54 [27200/50000 (54%)]\tLoss: 1.727325\tError: 57.812500\n",
      "Train Epoch: 0.54 [27264/50000 (54%)]\tLoss: 1.493943\tError: 57.812500\n",
      "Train Epoch: 0.54 [27328/50000 (54%)]\tLoss: 1.510611\tError: 48.437500\n",
      "Train Epoch: 0.55 [27392/50000 (55%)]\tLoss: 1.489362\tError: 48.437500\n",
      "Train Epoch: 0.55 [27456/50000 (55%)]\tLoss: 1.346864\tError: 56.250000\n",
      "Train Epoch: 0.55 [27520/50000 (55%)]\tLoss: 1.200641\tError: 45.312500\n",
      "Train Epoch: 0.55 [27584/50000 (55%)]\tLoss: 1.164903\tError: 40.625000\n",
      "Train Epoch: 0.55 [27648/50000 (55%)]\tLoss: 1.608813\tError: 54.687500\n",
      "Train Epoch: 0.55 [27712/50000 (55%)]\tLoss: 1.683155\tError: 65.625000\n",
      "Train Epoch: 0.55 [27776/50000 (55%)]\tLoss: 1.481142\tError: 50.000000\n",
      "Train Epoch: 0.55 [27840/50000 (55%)]\tLoss: 1.537014\tError: 56.250000\n",
      "Train Epoch: 0.56 [27904/50000 (56%)]\tLoss: 1.320465\tError: 46.875000\n",
      "Train Epoch: 0.56 [27968/50000 (56%)]\tLoss: 1.383469\tError: 45.312500\n",
      "Train Epoch: 0.56 [28032/50000 (56%)]\tLoss: 1.143238\tError: 37.500000\n",
      "Train Epoch: 0.56 [28096/50000 (56%)]\tLoss: 1.307398\tError: 45.312500\n",
      "Train Epoch: 0.56 [28160/50000 (56%)]\tLoss: 1.165693\tError: 40.625000\n",
      "Train Epoch: 0.56 [28224/50000 (56%)]\tLoss: 1.318064\tError: 46.875000\n",
      "Train Epoch: 0.56 [28288/50000 (56%)]\tLoss: 1.450121\tError: 53.125000\n",
      "Train Epoch: 0.57 [28352/50000 (57%)]\tLoss: 1.307552\tError: 51.562500\n",
      "Train Epoch: 0.57 [28416/50000 (57%)]\tLoss: 1.180576\tError: 50.000000\n",
      "Train Epoch: 0.57 [28480/50000 (57%)]\tLoss: 1.300571\tError: 40.625000\n",
      "Train Epoch: 0.57 [28544/50000 (57%)]\tLoss: 1.257455\tError: 43.750000\n",
      "Train Epoch: 0.57 [28608/50000 (57%)]\tLoss: 1.186534\tError: 43.750000\n",
      "Train Epoch: 0.57 [28672/50000 (57%)]\tLoss: 1.415666\tError: 46.875000\n",
      "Train Epoch: 0.57 [28736/50000 (57%)]\tLoss: 1.606636\tError: 46.875000\n",
      "Train Epoch: 0.57 [28800/50000 (57%)]\tLoss: 1.557396\tError: 53.125000\n",
      "Train Epoch: 0.58 [28864/50000 (58%)]\tLoss: 1.073925\tError: 31.250000\n",
      "Train Epoch: 0.58 [28928/50000 (58%)]\tLoss: 1.387701\tError: 50.000000\n",
      "Train Epoch: 0.58 [28992/50000 (58%)]\tLoss: 1.479399\tError: 59.375000\n",
      "Train Epoch: 0.58 [29056/50000 (58%)]\tLoss: 1.472186\tError: 53.125000\n",
      "Train Epoch: 0.58 [29120/50000 (58%)]\tLoss: 1.508204\tError: 51.562500\n",
      "Train Epoch: 0.58 [29184/50000 (58%)]\tLoss: 1.431491\tError: 54.687500\n",
      "Train Epoch: 0.58 [29248/50000 (58%)]\tLoss: 1.285712\tError: 40.625000\n",
      "Train Epoch: 0.58 [29312/50000 (58%)]\tLoss: 1.362359\tError: 53.125000\n",
      "Train Epoch: 0.59 [29376/50000 (59%)]\tLoss: 1.457446\tError: 51.562500\n",
      "Train Epoch: 0.59 [29440/50000 (59%)]\tLoss: 1.331942\tError: 48.437500\n",
      "Train Epoch: 0.59 [29504/50000 (59%)]\tLoss: 1.369958\tError: 57.812500\n",
      "Train Epoch: 0.59 [29568/50000 (59%)]\tLoss: 0.997606\tError: 37.500000\n",
      "Train Epoch: 0.59 [29632/50000 (59%)]\tLoss: 1.301130\tError: 50.000000\n",
      "Train Epoch: 0.59 [29696/50000 (59%)]\tLoss: 1.264510\tError: 48.437500\n",
      "Train Epoch: 0.59 [29760/50000 (59%)]\tLoss: 1.186254\tError: 37.500000\n",
      "Train Epoch: 0.59 [29824/50000 (59%)]\tLoss: 1.526540\tError: 51.562500\n",
      "Train Epoch: 0.60 [29888/50000 (60%)]\tLoss: 1.331069\tError: 48.437500\n",
      "Train Epoch: 0.60 [29952/50000 (60%)]\tLoss: 1.417462\tError: 53.125000\n",
      "Train Epoch: 0.60 [30016/50000 (60%)]\tLoss: 1.220365\tError: 51.562500\n",
      "Train Epoch: 0.60 [30080/50000 (60%)]\tLoss: 1.300914\tError: 46.875000\n",
      "Train Epoch: 0.60 [30144/50000 (60%)]\tLoss: 1.574421\tError: 56.250000\n",
      "Train Epoch: 0.60 [30208/50000 (60%)]\tLoss: 1.249349\tError: 46.875000\n",
      "Train Epoch: 0.60 [30272/50000 (60%)]\tLoss: 1.156896\tError: 43.750000\n",
      "Train Epoch: 0.60 [30336/50000 (60%)]\tLoss: 1.431387\tError: 56.250000\n",
      "Train Epoch: 0.61 [30400/50000 (61%)]\tLoss: 1.400788\tError: 56.250000\n",
      "Train Epoch: 0.61 [30464/50000 (61%)]\tLoss: 1.289662\tError: 46.875000\n",
      "Train Epoch: 0.61 [30528/50000 (61%)]\tLoss: 1.461706\tError: 50.000000\n",
      "Train Epoch: 0.61 [30592/50000 (61%)]\tLoss: 1.213768\tError: 51.562500\n",
      "Train Epoch: 0.61 [30656/50000 (61%)]\tLoss: 1.178756\tError: 40.625000\n",
      "Train Epoch: 0.61 [30720/50000 (61%)]\tLoss: 1.426329\tError: 48.437500\n",
      "Train Epoch: 0.61 [30784/50000 (61%)]\tLoss: 1.280121\tError: 48.437500\n",
      "Train Epoch: 0.62 [30848/50000 (62%)]\tLoss: 1.194156\tError: 42.187500\n",
      "Train Epoch: 0.62 [30912/50000 (62%)]\tLoss: 1.291995\tError: 50.000000\n",
      "Train Epoch: 0.62 [30976/50000 (62%)]\tLoss: 1.118602\tError: 34.375000\n",
      "Train Epoch: 0.62 [31040/50000 (62%)]\tLoss: 1.217819\tError: 35.937500\n",
      "Train Epoch: 0.62 [31104/50000 (62%)]\tLoss: 1.131608\tError: 37.500000\n",
      "Train Epoch: 0.62 [31168/50000 (62%)]\tLoss: 1.198006\tError: 37.500000\n",
      "Train Epoch: 0.62 [31232/50000 (62%)]\tLoss: 1.348593\tError: 53.125000\n",
      "Train Epoch: 0.62 [31296/50000 (62%)]\tLoss: 1.349825\tError: 48.437500\n",
      "Train Epoch: 0.63 [31360/50000 (63%)]\tLoss: 1.321515\tError: 51.562500\n",
      "Train Epoch: 0.63 [31424/50000 (63%)]\tLoss: 1.198798\tError: 43.750000\n",
      "Train Epoch: 0.63 [31488/50000 (63%)]\tLoss: 1.145676\tError: 39.062500\n",
      "Train Epoch: 0.63 [31552/50000 (63%)]\tLoss: 1.233163\tError: 45.312500\n",
      "Train Epoch: 0.63 [31616/50000 (63%)]\tLoss: 1.371988\tError: 53.125000\n",
      "Train Epoch: 0.63 [31680/50000 (63%)]\tLoss: 1.139657\tError: 42.187500\n",
      "Train Epoch: 0.63 [31744/50000 (63%)]\tLoss: 1.288848\tError: 45.312500\n",
      "Train Epoch: 0.63 [31808/50000 (63%)]\tLoss: 1.174727\tError: 34.375000\n",
      "Train Epoch: 0.64 [31872/50000 (64%)]\tLoss: 1.277743\tError: 42.187500\n",
      "Train Epoch: 0.64 [31936/50000 (64%)]\tLoss: 1.360189\tError: 59.375000\n",
      "Train Epoch: 0.64 [32000/50000 (64%)]\tLoss: 1.378910\tError: 50.000000\n",
      "Train Epoch: 0.64 [32064/50000 (64%)]\tLoss: 1.011858\tError: 42.187500\n",
      "Train Epoch: 0.64 [32128/50000 (64%)]\tLoss: 1.283562\tError: 45.312500\n",
      "Train Epoch: 0.64 [32192/50000 (64%)]\tLoss: 1.271128\tError: 42.187500\n",
      "Train Epoch: 0.64 [32256/50000 (64%)]\tLoss: 1.235516\tError: 54.687500\n",
      "Train Epoch: 0.64 [32320/50000 (64%)]\tLoss: 1.270315\tError: 50.000000\n",
      "Train Epoch: 0.65 [32384/50000 (65%)]\tLoss: 1.211559\tError: 42.187500\n",
      "Train Epoch: 0.65 [32448/50000 (65%)]\tLoss: 1.196453\tError: 45.312500\n",
      "Train Epoch: 0.65 [32512/50000 (65%)]\tLoss: 1.389609\tError: 46.875000\n",
      "Train Epoch: 0.65 [32576/50000 (65%)]\tLoss: 1.226658\tError: 42.187500\n",
      "Train Epoch: 0.65 [32640/50000 (65%)]\tLoss: 1.447052\tError: 42.187500\n",
      "Train Epoch: 0.65 [32704/50000 (65%)]\tLoss: 1.140575\tError: 42.187500\n",
      "Train Epoch: 0.65 [32768/50000 (65%)]\tLoss: 1.478199\tError: 53.125000\n",
      "Train Epoch: 0.65 [32832/50000 (65%)]\tLoss: 1.101980\tError: 48.437500\n",
      "Train Epoch: 0.66 [32896/50000 (66%)]\tLoss: 1.227797\tError: 43.750000\n",
      "Train Epoch: 0.66 [32960/50000 (66%)]\tLoss: 1.237864\tError: 37.500000\n",
      "Train Epoch: 0.66 [33024/50000 (66%)]\tLoss: 1.357885\tError: 51.562500\n",
      "Train Epoch: 0.66 [33088/50000 (66%)]\tLoss: 1.184612\tError: 34.375000\n",
      "Train Epoch: 0.66 [33152/50000 (66%)]\tLoss: 1.136072\tError: 45.312500\n",
      "Train Epoch: 0.66 [33216/50000 (66%)]\tLoss: 1.137935\tError: 39.062500\n",
      "Train Epoch: 0.66 [33280/50000 (66%)]\tLoss: 1.244479\tError: 43.750000\n",
      "Train Epoch: 0.66 [33344/50000 (66%)]\tLoss: 1.160166\tError: 45.312500\n",
      "Train Epoch: 0.67 [33408/50000 (67%)]\tLoss: 1.259350\tError: 50.000000\n",
      "Train Epoch: 0.67 [33472/50000 (67%)]\tLoss: 1.124647\tError: 34.375000\n",
      "Train Epoch: 0.67 [33536/50000 (67%)]\tLoss: 1.059321\tError: 35.937500\n",
      "Train Epoch: 0.67 [33600/50000 (67%)]\tLoss: 1.439157\tError: 56.250000\n",
      "Train Epoch: 0.67 [33664/50000 (67%)]\tLoss: 1.043252\tError: 43.750000\n",
      "Train Epoch: 0.67 [33728/50000 (67%)]\tLoss: 1.216302\tError: 45.312500\n",
      "Train Epoch: 0.67 [33792/50000 (67%)]\tLoss: 1.225408\tError: 43.750000\n",
      "Train Epoch: 0.68 [33856/50000 (68%)]\tLoss: 1.513592\tError: 48.437500\n",
      "Train Epoch: 0.68 [33920/50000 (68%)]\tLoss: 1.343076\tError: 45.312500\n",
      "Train Epoch: 0.68 [33984/50000 (68%)]\tLoss: 1.458114\tError: 56.250000\n",
      "Train Epoch: 0.68 [34048/50000 (68%)]\tLoss: 1.386843\tError: 50.000000\n",
      "Train Epoch: 0.68 [34112/50000 (68%)]\tLoss: 1.250632\tError: 48.437500\n",
      "Train Epoch: 0.68 [34176/50000 (68%)]\tLoss: 1.295805\tError: 50.000000\n",
      "Train Epoch: 0.68 [34240/50000 (68%)]\tLoss: 1.101784\tError: 45.312500\n",
      "Train Epoch: 0.68 [34304/50000 (68%)]\tLoss: 1.216493\tError: 45.312500\n",
      "Train Epoch: 0.69 [34368/50000 (69%)]\tLoss: 1.367210\tError: 54.687500\n",
      "Train Epoch: 0.69 [34432/50000 (69%)]\tLoss: 1.045155\tError: 42.187500\n",
      "Train Epoch: 0.69 [34496/50000 (69%)]\tLoss: 1.411059\tError: 50.000000\n",
      "Train Epoch: 0.69 [34560/50000 (69%)]\tLoss: 1.483815\tError: 46.875000\n",
      "Train Epoch: 0.69 [34624/50000 (69%)]\tLoss: 1.469827\tError: 59.375000\n",
      "Train Epoch: 0.69 [34688/50000 (69%)]\tLoss: 1.282633\tError: 46.875000\n",
      "Train Epoch: 0.69 [34752/50000 (69%)]\tLoss: 1.177356\tError: 45.312500\n",
      "Train Epoch: 0.69 [34816/50000 (69%)]\tLoss: 1.294928\tError: 42.187500\n",
      "Train Epoch: 0.70 [34880/50000 (70%)]\tLoss: 1.102993\tError: 43.750000\n",
      "Train Epoch: 0.70 [34944/50000 (70%)]\tLoss: 1.074852\tError: 34.375000\n",
      "Train Epoch: 0.70 [35008/50000 (70%)]\tLoss: 1.107989\tError: 40.625000\n",
      "Train Epoch: 0.70 [35072/50000 (70%)]\tLoss: 1.245192\tError: 42.187500\n",
      "Train Epoch: 0.70 [35136/50000 (70%)]\tLoss: 1.192125\tError: 46.875000\n",
      "Train Epoch: 0.70 [35200/50000 (70%)]\tLoss: 1.374709\tError: 51.562500\n",
      "Train Epoch: 0.70 [35264/50000 (70%)]\tLoss: 1.557056\tError: 56.250000\n",
      "Train Epoch: 0.70 [35328/50000 (70%)]\tLoss: 1.161721\tError: 42.187500\n",
      "Train Epoch: 0.71 [35392/50000 (71%)]\tLoss: 1.075835\tError: 43.750000\n",
      "Train Epoch: 0.71 [35456/50000 (71%)]\tLoss: 1.179693\tError: 42.187500\n",
      "Train Epoch: 0.71 [35520/50000 (71%)]\tLoss: 1.483200\tError: 50.000000\n",
      "Train Epoch: 0.71 [35584/50000 (71%)]\tLoss: 1.025078\tError: 34.375000\n",
      "Train Epoch: 0.71 [35648/50000 (71%)]\tLoss: 1.229325\tError: 46.875000\n",
      "Train Epoch: 0.71 [35712/50000 (71%)]\tLoss: 1.436432\tError: 51.562500\n",
      "Train Epoch: 0.71 [35776/50000 (71%)]\tLoss: 1.496876\tError: 62.500000\n",
      "Train Epoch: 0.71 [35840/50000 (71%)]\tLoss: 1.250837\tError: 34.375000\n",
      "Train Epoch: 0.72 [35904/50000 (72%)]\tLoss: 1.404642\tError: 53.125000\n",
      "Train Epoch: 0.72 [35968/50000 (72%)]\tLoss: 0.937038\tError: 28.125000\n",
      "Train Epoch: 0.72 [36032/50000 (72%)]\tLoss: 1.038661\tError: 34.375000\n",
      "Train Epoch: 0.72 [36096/50000 (72%)]\tLoss: 1.185904\tError: 40.625000\n",
      "Train Epoch: 0.72 [36160/50000 (72%)]\tLoss: 1.329630\tError: 43.750000\n",
      "Train Epoch: 0.72 [36224/50000 (72%)]\tLoss: 1.241193\tError: 39.062500\n",
      "Train Epoch: 0.72 [36288/50000 (72%)]\tLoss: 1.276179\tError: 48.437500\n",
      "Train Epoch: 0.73 [36352/50000 (73%)]\tLoss: 1.459965\tError: 46.875000\n",
      "Train Epoch: 0.73 [36416/50000 (73%)]\tLoss: 1.141264\tError: 42.187500\n",
      "Train Epoch: 0.73 [36480/50000 (73%)]\tLoss: 1.059080\tError: 35.937500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3q/jdpvjfp16290w3y4kytswtfhfy5pvf/T/ipykernel_14350/3710550220.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnEpochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0madjust_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latest.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3q/jdpvjfp16290w3y4kytswtfhfy5pvf/T/ipykernel_14350/1438850636.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, epoch, net, trainLoader, optimizer, trainF)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# make_graph.save('/tmp/t.dot', loss.creator); assert(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnProcessed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('  Total Number of params: {}'.format(\n",
    "        sum([p.data.nelement() for p in net.parameters()])))\n",
    "\n",
    "if args.cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "if args.opt == 'sgd':\n",
    "    optimizer = optim.SGD(net.parameters(), lr=1e-1,\n",
    "                        momentum=0.9, weight_decay=1e-4)\n",
    "elif args.opt == 'adam':\n",
    "    optimizer = optim.Adam(net.parameters(), weight_decay=1e-4)\n",
    "elif args.opt == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(net.parameters(), weight_decay=1e-4)\n",
    "    \n",
    "# the 'w' here ensure creation of the file if it didn't exit already\n",
    "trainF = open(os.path.join(args.save, 'train.csv'), 'w')\n",
    "testF = open(os.path.join(args.save, 'test.csv'), 'w')\n",
    "\n",
    "for epoch in range(1, args.nEpochs + 1):\n",
    "    adjust_opt(args.opt, optimizer, epoch)\n",
    "    train(args, epoch, net, trainLoader, optimizer, trainF)\n",
    "    test(args, epoch, net, testLoader, optimizer, testF)\n",
    "    torch.save(net, os.path.join(args.save, 'latest.pth'))\n",
    "    os.system('./plot.py {} &'.format(args.save))\n",
    "\n",
    "trainF.close()\n",
    "testF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de11333-1e54-4cd7-80d0-ff2b17c9eb37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
